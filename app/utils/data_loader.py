"""
Streamlit ìºì‹œ ë°ì´í„° ë¡œë”

DB ì—°ê²°, Stage 1-3 ë¶„ì„ íŒŒì´í”„ë¼ì¸, ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ì„ ìºì‹±í•˜ì—¬ ì„±ëŠ¥ í™•ë³´.
ê¸°ì¡´ ëª¨ë“ˆ(normalizer, pattern_classifier ë“±)ì„ ìˆ˜ì • ì—†ì´ ì¬ì‚¬ìš©.
"""

import sqlite3
import sys
from pathlib import Path
from typing import Optional, Tuple, List, Dict, Any

import pandas as pd
import streamlit as st

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ë“±ë¡
_PROJECT_ROOT = Path(__file__).parent.parent.parent
if str(_PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(_PROJECT_ROOT))

from src.config import DEFAULT_CONFIG
from src.database.connection import DB_PATH
from src.analyzer.normalizer import SupplyNormalizer
from src.visualizer.performance_optimizer import OptimizedMultiPeriodCalculator
from src.analyzer.pattern_classifier import PatternClassifier
from src.analyzer.signal_detector import SignalDetector
from src.analyzer.integrated_report import IntegratedReport
from src.backtesting.engine import BacktestConfig, BacktestEngine
from src.backtesting.metrics import PerformanceMetrics
from src.backtesting.portfolio import Trade


# ---------------------------------------------------------------------------
# DB ì—°ê²° (ì‹±ê¸€í„´)
# ---------------------------------------------------------------------------

@st.cache_resource
def get_db_connection() -> sqlite3.Connection:
    """Streamlit ìŠ¤ë ˆë“œ ì•ˆì „ DB ì—°ê²° (check_same_thread=False)"""
    db_path = str(_PROJECT_ROOT / DB_PATH)
    conn = sqlite3.connect(db_path, check_same_thread=False)
    conn.row_factory = sqlite3.Row
    conn.execute('PRAGMA foreign_keys = ON')
    return conn


# ---------------------------------------------------------------------------
# ì •ì  ë°ì´í„° (ì¢…ëª©/ì„¹í„°/ë‚ ì§œ ë²”ìœ„)
# ---------------------------------------------------------------------------

@st.cache_data(ttl=3600)
def get_stock_list() -> pd.DataFrame:
    """ì¢…ëª© ë¦¬ìŠ¤íŠ¸ (stock_code, stock_name, sector)"""
    conn = get_db_connection()
    df = pd.read_sql_query(
        "SELECT stock_code, stock_name, sector FROM stocks ORDER BY stock_code",
        conn,
    )
    return df


@st.cache_data(ttl=3600)
def get_sectors() -> List[str]:
    """ê³ ìœ  ì„¹í„° ëª©ë¡"""
    conn = get_db_connection()
    rows = conn.execute(
        "SELECT DISTINCT sector FROM stocks WHERE sector IS NOT NULL ORDER BY sector"
    ).fetchall()
    return [r[0] for r in rows]


@st.cache_data(ttl=3600)
def get_date_range() -> Tuple[str, str]:
    """DB ë‚´ ê±°ë˜ ë‚ ì§œ ë²”ìœ„ (min_date, max_date)"""
    conn = get_db_connection()
    row = conn.execute(
        "SELECT MIN(trade_date), MAX(trade_date) FROM investor_flows"
    ).fetchone()
    return row[0], row[1]


# ---------------------------------------------------------------------------
# ì´ìƒ ìˆ˜ê¸‰ íƒì§€ (ìºì‹±)
# ---------------------------------------------------------------------------

@st.cache_data(ttl=600, show_spinner=False)
def get_today_supply_ranking(top_n: int = 50) -> pd.DataFrame:
    """ë‹¹ì¼ ì „ ì¢…ëª© ì™¸êµ­ì¸/ê¸°ê´€ ìˆœë§¤ìˆ˜ê¸ˆì•¡ ì¡°íšŒ (ìºì‹±)"""
    conn = get_db_connection()
    max_date = conn.execute(
        "SELECT MAX(trade_date) FROM investor_flows"
    ).fetchone()[0]
    df = pd.read_sql_query(
        "SELECT f.stock_code, s.stock_name, s.sector, "
        "f.foreign_net_amount, f.institution_net_amount "
        "FROM investor_flows f "
        "JOIN stocks s ON f.stock_code = s.stock_code "
        "WHERE f.trade_date = ?",
        conn,
        params=[max_date],
    )
    return df


@st.cache_data(ttl=600, show_spinner=False)
def get_abnormal_supply_data(
    end_date: Optional[str] = None,
    threshold: float = 2.0,
    top_n: int = 10,
    direction: str = 'both',
    institution_weight: float = 0.3,
    z_score_window: int = 60,
) -> pd.DataFrame:
    """ì´ìƒ ìˆ˜ê¸‰ ì¢…ëª© ì¡°íšŒ (ìºì‹±) â€” ìˆœë§¤ìˆ˜ê¸ˆì•¡ í¬í•¨"""
    conn = get_db_connection()
    normalizer = SupplyNormalizer(conn, config={
        'z_score_window': z_score_window,
        'min_data_points': 30,
        'institution_weight': institution_weight,
    })
    df = normalizer.get_abnormal_supply(
        threshold=threshold,
        end_date=end_date,
        top_n=top_n,
        direction=direction,
    )
    if df.empty:
        return df

    # ìˆœë§¤ìˆ˜ê¸ˆì•¡ ì¡°ì¸
    trade_date = df['trade_date'].iloc[0]
    codes = df['stock_code'].tolist()
    placeholders = ','.join('?' for _ in codes)
    amounts = pd.read_sql_query(
        f"SELECT stock_code, foreign_net_amount, institution_net_amount "
        f"FROM investor_flows WHERE trade_date = ? AND stock_code IN ({placeholders})",
        conn,
        params=[trade_date] + codes,
    )
    df = df.merge(amounts, on='stock_code', how='left')
    return df


# ---------------------------------------------------------------------------
# Stage 1-3 ë¶„ì„ íŒŒì´í”„ë¼ì¸ (ë‹¨ê³„ë³„ ìºì‹œ ë¶„ë¦¬)
# ---------------------------------------------------------------------------

@st.cache_data(ttl=600, show_spinner=False)
def _stage_zscore(end_date: Optional[str] = None, institution_weight: float = 0.3) -> pd.DataFrame:
    """Stage 1+2: ìˆ˜ê¸‰ ì •ê·œí™” + ë©€í‹° ê¸°ê°„ Z-Score"""
    conn = get_db_connection()
    normalizer = SupplyNormalizer(conn, config={
        'z_score_window': 60,
        'min_data_points': 30,
        'institution_weight': institution_weight,
    })
    calculator = OptimizedMultiPeriodCalculator(normalizer, enable_caching=True)
    zscore_matrix = calculator.calculate_multi_period_zscores(
        DEFAULT_CONFIG['periods'], end_date=end_date
    )
    return zscore_matrix.reset_index()


@st.cache_data(ttl=600, show_spinner=False)
def _stage_classify(end_date: Optional[str] = None, institution_weight: float = 0.3) -> pd.DataFrame:
    """Stage 3a: íŒ¨í„´ ë¶„ë¥˜"""
    zscore_matrix = _stage_zscore(end_date=end_date, institution_weight=institution_weight)
    if zscore_matrix.empty:
        return pd.DataFrame()
    classifier = PatternClassifier()
    return classifier.classify_all(zscore_matrix)


@st.cache_data(ttl=600, show_spinner=False)
def _stage_signals(end_date: Optional[str] = None, institution_weight: float = 0.3) -> pd.DataFrame:
    """Stage 3b: ì‹œê·¸ë„ íƒì§€"""
    conn = get_db_connection()
    detector = SignalDetector(conn, institution_weight=institution_weight)
    return detector.detect_all_signals(end_date=end_date)


@st.cache_data(ttl=600, show_spinner=False)
def _stage_report(end_date: Optional[str] = None, institution_weight: float = 0.3) -> pd.DataFrame:
    """Stage 3c: í†µí•© ë¦¬í¬íŠ¸"""
    classified_df = _stage_classify(end_date=end_date, institution_weight=institution_weight)
    signals_df = _stage_signals(end_date=end_date, institution_weight=institution_weight)
    if classified_df.empty:
        return pd.DataFrame()
    conn = get_db_connection()
    report_gen = IntegratedReport(conn)
    return report_gen.generate_report(classified_df, signals_df)


def run_analysis_pipeline(
    end_date: Optional[str] = None,
    institution_weight: float = 0.3,
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """Stage 1-3 ì „ì²´ íŒŒì´í”„ë¼ì¸ (progress bar ì—†ëŠ” ë²„ì „)"""
    return run_analysis_pipeline_with_progress(
        end_date=end_date, progress_bar=None,
        institution_weight=institution_weight,
    )


def run_analysis_pipeline_with_progress(
    end_date: Optional[str] = None,
    progress_bar=None,
    institution_weight: float = 0.3,
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Stage 1-3 ì „ì²´ íŒŒì´í”„ë¼ì¸ (ë‹¨ê³„ë³„ ì§„í–‰ë¥  í‘œì‹œ ì§€ì›)

    Args:
        end_date: ë¶„ì„ ê¸°ì¤€ ë‚ ì§œ
        progress_bar: st.progress ìœ„ì ¯ (Noneì´ë©´ ì§„í–‰ë¥  í‘œì‹œ ì•ˆ í•¨)
        institution_weight: ê¸°ê´€ ê°€ì¤‘ì¹˜ (0.0=ì™¸êµ­ì¸ë§Œ, 0.3=ê¸°ë³¸, 1.0=ë™ë“±)

    Returns:
        (zscore_matrix, classified_df, signals_df, report_df)
    """
    def _upd(pct: float, msg: str):
        if progress_bar is not None:
            progress_bar.progress(pct, text=msg)

    _upd(0.05, "ğŸ“ ìˆ˜ê¸‰ ë°ì´í„° ì •ê·œí™” ì¤‘... 5%")
    zscore_matrix = _stage_zscore(end_date=end_date, institution_weight=institution_weight)

    if zscore_matrix.empty:
        _upd(1.0, "âœ… ì™„ë£Œ 100%")
        empty = pd.DataFrame()
        return zscore_matrix, empty, empty, empty

    _upd(0.40, "ğŸ“Š Z-Score ê³„ì‚° ì™„ë£Œ â†’ íŒ¨í„´ ë¶„ë¥˜ ì¤‘... 40%")
    classified_df = _stage_classify(end_date=end_date, institution_weight=institution_weight)

    _upd(0.65, "ğŸ” íŒ¨í„´ ë¶„ë¥˜ ì™„ë£Œ â†’ ì‹œê·¸ë„ íƒì§€ ì¤‘... 65%")
    signals_df = _stage_signals(end_date=end_date, institution_weight=institution_weight)

    _upd(0.75, "ğŸ“¡ ì‹œê·¸ë„ íƒì§€ ì™„ë£Œ â†’ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘... 75%")
    report_df = _stage_report(end_date=end_date, institution_weight=institution_weight)

    _upd(0.85, "ğŸ“‹ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ 85%")
    return zscore_matrix, classified_df, signals_df, report_df


# ---------------------------------------------------------------------------
# ì¢…ëª© ìƒì„¸ ë°ì´í„°
# ---------------------------------------------------------------------------

@st.cache_data(ttl=600, show_spinner=False)
def get_stock_zscore_history(
    stock_code: str,
    end_date: Optional[str] = None,
    institution_weight: float = 0.3,
    z_score_window: int = 60,
) -> pd.DataFrame:
    """ë‹¨ì¼ ì¢…ëª©ì˜ Z-Score ì „ì²´ ì‹œê³„ì—´ ì´ë ¥ ë°˜í™˜

    Returns:
        ì»¬ëŸ¼: trade_date, stock_code, foreign_zscore, institution_zscore, combined_zscore
    """
    conn = get_db_connection()
    normalizer = SupplyNormalizer(conn, config={
        'z_score_window': z_score_window,
        'min_data_points': min(30, z_score_window // 2),
        'institution_weight': institution_weight,
    })
    return normalizer.calculate_zscore(stock_codes=[stock_code], end_date=end_date)


@st.cache_data(ttl=600, show_spinner=False)
def get_stock_raw_history(
    stock_code: str,
    end_date: Optional[str] = None,
) -> pd.DataFrame:
    """ë‹¨ì¼ ì¢…ëª©ì˜ ì›ì‹œ ìˆ˜ê¸‰+ê°€ê²© ì´ë ¥ + íŒŒìƒ ì§€í‘œ

    Returns:
        ì»¬ëŸ¼: trade_date, close_price, foreign_net_amount, institution_net_amount,
               trading_volume, ma5, ma20, sync_rate
    """
    conn = get_db_connection()
    if end_date:
        query = (
            "SELECT trade_date, close_price, foreign_net_amount, institution_net_amount, "
            "trading_volume FROM investor_flows "
            "WHERE stock_code = ? AND trade_date <= ? ORDER BY trade_date"
        )
        df = pd.read_sql_query(query, conn, params=[stock_code, end_date])
    else:
        query = (
            "SELECT trade_date, close_price, foreign_net_amount, institution_net_amount, "
            "trading_volume FROM investor_flows "
            "WHERE stock_code = ? ORDER BY trade_date"
        )
        df = pd.read_sql_query(query, conn, params=[stock_code])

    if df.empty:
        return df

    # ê°œì¸ ìˆœë§¤ìˆ˜ (ì™¸êµ­ì¸+ê¸°ê´€+ê°œì¸ í•©ê³„ â‰ˆ 0 ì›ë¦¬)
    df['individual_net_amount'] = -(df['foreign_net_amount'] + df['institution_net_amount'])

    # íŒŒìƒ ì§€í‘œ (ì™¸êµ­ì¸ ìˆœë§¤ìˆ˜ ê¸°ì¤€ ì´ë™í‰ê· )
    df['ma5']  = df['foreign_net_amount'].rolling(5).mean()
    df['ma20'] = df['foreign_net_amount'].rolling(20).mean()

    both_buy = (
        (df['foreign_net_amount'] > 0) & (df['institution_net_amount'] > 0)
    ).astype(int)
    df['sync_rate'] = both_buy.rolling(20).mean() * 100

    return df


# ---------------------------------------------------------------------------
# ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# ---------------------------------------------------------------------------

def _serialize_trades(trades: List[Trade]) -> List[dict]:
    """Trade ê°ì²´ ë¦¬ìŠ¤íŠ¸ â†’ dict ë¦¬ìŠ¤íŠ¸ (ìºì‹± ê°€ëŠ¥ í˜•íƒœ)"""
    result = []
    for t in trades:
        d = t.to_dict()
        # to_dict()ì— profit (property) í¬í•¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œê±°
        d.pop('profit', None)
        result.append(d)
    return result


def _deserialize_trades(trade_dicts: List[dict]) -> List[Trade]:
    """dict ë¦¬ìŠ¤íŠ¸ â†’ Trade ê°ì²´ ë¦¬ìŠ¤íŠ¸"""
    trades = []
    for d in trade_dicts:
        d_clean = {k: v for k, v in d.items() if k != 'profit'}
        trades.append(Trade(**d_clean))
    return trades


@st.cache_data(ttl=300, show_spinner="ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
def run_backtest(
    start_date: str,
    end_date: str,
    strategy: str = 'long',
    min_score: float = 60,
    min_signals: int = 1,
    target_return: float = 0.15,
    stop_loss: float = -0.075,
    max_hold_days: int = 999,
    initial_capital: float = 10_000_000,
    max_positions: int = 5,
    institution_weight: float = 0.3,
    reverse_threshold: float = 60,
    allowed_patterns: Optional[List[str]] = None,
    tax_rate: float = 0.0020,
    commission_rate: float = 0.00015,
    slippage_rate: float = 0.001,
    borrowing_rate: float = 0.03,
    use_tc: bool = True,
    use_divergence: bool = True,
) -> Dict:
    """
    ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ìºì‹±)

    Returns:
        {
            'trade_dicts': List[dict],
            'daily_values': DataFrame,
            'config': dict,
            'initial_capital': float,
        }
    """
    conn = get_db_connection()

    config = BacktestConfig(
        initial_capital=initial_capital,
        max_positions=max_positions,
        min_score=min_score,
        min_signals=min_signals,
        target_return=target_return,
        stop_loss=stop_loss,
        max_hold_days=max_hold_days,
        reverse_signal_threshold=reverse_threshold,
        allowed_patterns=allowed_patterns,
        strategy=strategy,
        institution_weight=institution_weight,
        force_exit_on_end=True,
        use_tc=use_tc,
        use_divergence=use_divergence,
        tax_rate=tax_rate,
        commission_rate=commission_rate,
        slippage_rate=slippage_rate,
        borrowing_rate=borrowing_rate,
    )

    engine = BacktestEngine(conn, config)
    result = engine.run(
        start_date=start_date,
        end_date=end_date,
        verbose=False,
    )

    return {
        'trade_dicts': _serialize_trades(result['trades']),
        'daily_values': result['daily_values'],
        'config': {
            'initial_capital': config.initial_capital,
            'max_positions': config.max_positions,
            'min_score': config.min_score,
            'min_signals': config.min_signals,
            'target_return': config.target_return,
            'stop_loss': config.stop_loss,
            'max_hold_days': config.max_hold_days,
            'reverse_signal_threshold': config.reverse_signal_threshold,
            'strategy': config.strategy,
            'institution_weight': config.institution_weight,
            'use_tc': config.use_tc,
            'use_divergence': config.use_divergence,
            'tax_rate': config.tax_rate,
            'commission_rate': config.commission_rate,
            'slippage_rate': config.slippage_rate,
            'borrowing_rate': config.borrowing_rate,
        },
        'initial_capital': config.initial_capital,
    }


def get_metrics_from_result(result: Dict) -> Optional[PerformanceMetrics]:
    """ìºì‹±ëœ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ì—ì„œ PerformanceMetrics ìƒì„±"""
    trades = _deserialize_trades(result['trade_dicts'])
    if not trades:
        return None
    return PerformanceMetrics(
        trades=trades,
        daily_values=result['daily_values'],
        initial_capital=result['initial_capital'],
    )


def get_trades_from_result(result: Dict) -> List[Trade]:
    """ìºì‹±ëœ ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ì—ì„œ Trade ë¦¬ìŠ¤íŠ¸ ë³µì›"""
    return _deserialize_trades(result['trade_dicts'])


def run_backtest_with_progress(
    start_date: str,
    end_date: str,
    strategy: str = 'long',
    min_score: float = 60,
    min_signals: int = 1,
    target_return: float = 0.15,
    stop_loss: float = -0.075,
    max_hold_days: int = 999,
    initial_capital: float = 10_000_000,
    max_positions: int = 5,
    institution_weight: float = 0.3,
    reverse_threshold: float = 60,
    allowed_patterns=None,
    progress_callback=None,
    tax_rate: float = 0.0020,
    commission_rate: float = 0.00015,
    slippage_rate: float = 0.001,
    borrowing_rate: float = 0.03,
    use_tc: bool = True,
    use_divergence: bool = True,
) -> Dict:
    """ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ìºì‹œ ì—†ìŒ, progress_callback ì§€ì›)"""
    conn = get_db_connection()
    config = BacktestConfig(
        initial_capital=initial_capital,
        max_positions=max_positions,
        min_score=min_score,
        min_signals=min_signals,
        target_return=target_return,
        stop_loss=stop_loss,
        max_hold_days=max_hold_days,
        reverse_signal_threshold=reverse_threshold,
        allowed_patterns=allowed_patterns,
        strategy=strategy,
        institution_weight=institution_weight,
        force_exit_on_end=True,
        use_tc=use_tc,
        use_divergence=use_divergence,
        tax_rate=tax_rate,
        commission_rate=commission_rate,
        slippage_rate=slippage_rate,
        borrowing_rate=borrowing_rate,
    )
    engine = BacktestEngine(conn, config)
    result = engine.run(
        start_date=start_date,
        end_date=end_date,
        verbose=False,
        progress_callback=progress_callback,
    )
    return {
        'trade_dicts': _serialize_trades(result['trades']),
        'daily_values': result['daily_values'],
        'config': {
            'initial_capital': config.initial_capital,
            'max_positions': config.max_positions,
            'min_score': config.min_score,
            'min_signals': config.min_signals,
            'target_return': config.target_return,
            'stop_loss': config.stop_loss,
            'max_hold_days': config.max_hold_days,
            'reverse_signal_threshold': config.reverse_signal_threshold,
            'strategy': config.strategy,
            'institution_weight': config.institution_weight,
            'use_tc': config.use_tc,
            'use_divergence': config.use_divergence,
            'tax_rate': config.tax_rate,
            'commission_rate': config.commission_rate,
            'slippage_rate': config.slippage_rate,
            'borrowing_rate': config.borrowing_rate,
        },
        'initial_capital': config.initial_capital,
    }


# ---------------------------------------------------------------------------
# Optuna ìµœì í™”
# ---------------------------------------------------------------------------

# Optuna study ëˆ„ì  ì €ì¥ ê²½ë¡œ (ë©”ì¸ DBì™€ ë³„ê°œì˜ ê²½ëŸ‰ SQLite)
_OPTUNA_STORAGE = f"sqlite:///{_PROJECT_ROOT / 'data' / 'optuna_studies.db'}"


def get_optuna_trial_count(
    start_date: str,
    end_date: str,
    strategy: str = 'long',
    metric: str = 'sharpe_ratio',
) -> int:
    """ì €ì¥ëœ Optuna studyì˜ ëˆ„ì  ì™„ë£Œ Trial ìˆ˜ ë°˜í™˜ (study ì—†ìœ¼ë©´ 0)"""
    try:
        import optuna
        optuna.logging.set_verbosity(optuna.logging.WARNING)
        sd = start_date.replace('-', '')
        ed = end_date.replace('-', '')
        study_name = f"opt__{strategy}__{sd}__{ed}__{metric}"
        study = optuna.load_study(study_name=study_name, storage=_OPTUNA_STORAGE)
        return sum(1 for t in study.trials if t.state.name == 'COMPLETE')
    except Exception:
        return 0


def run_optuna_optimization(
    start_date: str,
    end_date: str,
    strategy: str = 'long',
    n_trials: int = 100,
    metric: str = 'sharpe_ratio',
    initial_capital: float = 10_000_000,
    max_positions: int = 5,
    max_hold_days: int = 999,
    reverse_threshold: float = 60,
    institution_weight: float = 0.3,
    progress_callback=None,
    reset_study: bool = False,
    tax_rate: float = 0.0020,
    commission_rate: float = 0.00015,
    slippage_rate: float = 0.001,
    borrowing_rate: float = 0.03,
    use_tc: bool = True,
    use_divergence: bool = True,
) -> Optional[Dict]:
    """
    Optuna Persistent Bayesian Optimization ì‹¤í–‰

    ë™ì¼ ê¸°ê°„+ì „ëµ+ë©”íŠ¸ë¦­ìœ¼ë¡œ ì¬ì‹¤í–‰ ì‹œ ì´ì „ Trial ìœ„ì— ëˆ„ì  íƒìƒ‰.
    ì‹¤í–‰ íšŸìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ìµœê³ ê°’ì´ ë‹¨ì¡° ì¦ê°€(â‰¥)í•¨ì„ ë³´ì¥.

    Returns:
        {
            'params': dict,
            metric: float,
            'total_complete': int,  # ëˆ„ì  ì™„ë£Œ Trial
            'total_pruned': int,
            'existing_before': int, # ì´ë²ˆ ì‹¤í–‰ ì „ ëˆ„ì  ìˆ˜
        }
        ë˜ëŠ” None (ì™„ë£Œëœ Trialì´ ì—†ì„ ë•Œ)
    """
    from src.backtesting.optimizer import OptunaOptimizer

    db_path = str(_PROJECT_ROOT / DB_PATH)
    base_config = BacktestConfig(
        strategy=strategy,
        initial_capital=initial_capital,
        max_positions=max_positions,
        max_hold_days=max_hold_days,
        reverse_signal_threshold=reverse_threshold,
        institution_weight=institution_weight,
        force_exit_on_end=True,
        use_tc=use_tc,
        use_divergence=use_divergence,
        tax_rate=tax_rate,
        commission_rate=commission_rate,
        slippage_rate=slippage_rate,
        borrowing_rate=borrowing_rate,
    )

    optimizer = OptunaOptimizer(
        db_path=db_path,
        start_date=start_date,
        end_date=end_date,
        base_config=base_config,
        study_storage=_OPTUNA_STORAGE,
    )

    return optimizer.optimize(
        n_trials=n_trials,
        metric=metric,
        verbose=False,
        progress_callback=progress_callback,
        reset=reset_study,
    )


# ---------------------------------------------------------------------------
# ê´€ì‹¬ì¢…ëª© (Watchlist)
# ---------------------------------------------------------------------------

def _ensure_watchlist_table() -> None:
    """watchlist í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„± (ì•± ìµœì´ˆ ì‹¤í–‰ ì‹œ ìë™ í˜¸ì¶œ)"""
    db_path = str(_PROJECT_ROOT / DB_PATH)
    conn = sqlite3.connect(db_path)
    conn.execute('''
        CREATE TABLE IF NOT EXISTS watchlist (
            stock_code TEXT PRIMARY KEY,
            stock_name TEXT NOT NULL,
            sector     TEXT,
            added_at   TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            note       TEXT DEFAULT ''
        )
    ''')
    conn.commit()
    conn.close()


# ëª¨ë“ˆ ì„í¬íŠ¸ ì‹œ í…Œì´ë¸” ìë™ ìƒì„±
_ensure_watchlist_table()


def get_watchlist() -> pd.DataFrame:
    """ê´€ì‹¬ì¢…ëª© ëª©ë¡ ë°˜í™˜ (stock_code, stock_name, sector, added_at, note)"""
    db_path = str(_PROJECT_ROOT / DB_PATH)
    conn = sqlite3.connect(db_path)
    df = pd.read_sql_query(
        "SELECT stock_code, stock_name, sector, added_at, note FROM watchlist ORDER BY added_at DESC",
        conn,
    )
    conn.close()
    return df


def is_in_watchlist(stock_code: str) -> bool:
    """í•´ë‹¹ ì¢…ëª©ì´ ê´€ì‹¬ì¢…ëª©ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    db_path = str(_PROJECT_ROOT / DB_PATH)
    conn = sqlite3.connect(db_path)
    cursor = conn.execute(
        "SELECT 1 FROM watchlist WHERE stock_code = ?", (stock_code,)
    )
    result = cursor.fetchone() is not None
    conn.close()
    return result


def add_to_watchlist(stock_code: str, stock_name: str, sector: str = '', note: str = '') -> None:
    """ê´€ì‹¬ì¢…ëª© ì¶”ê°€ (ì´ë¯¸ ìˆìœ¼ë©´ ë¬´ì‹œ)"""
    db_path = str(_PROJECT_ROOT / DB_PATH)
    conn = sqlite3.connect(db_path)
    conn.execute(
        "INSERT OR IGNORE INTO watchlist (stock_code, stock_name, sector, note) VALUES (?, ?, ?, ?)",
        (stock_code, stock_name, sector or '', note),
    )
    conn.commit()
    conn.close()


def remove_from_watchlist(stock_code: str) -> None:
    """ê´€ì‹¬ì¢…ëª© ì‚­ì œ"""
    db_path = str(_PROJECT_ROOT / DB_PATH)
    conn = sqlite3.connect(db_path)
    conn.execute("DELETE FROM watchlist WHERE stock_code = ?", (stock_code,))
    conn.commit()
    conn.close()


def update_watchlist_note(stock_code: str, note: str) -> None:
    """ê´€ì‹¬ì¢…ëª© ë©”ëª¨ ìˆ˜ì •"""
    db_path = str(_PROJECT_ROOT / DB_PATH)
    conn = sqlite3.connect(db_path)
    conn.execute(
        "UPDATE watchlist SET note = ? WHERE stock_code = ?", (note, stock_code)
    )
    conn.commit()
    conn.close()


# ---------------------------------------------------------------------------
# ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ íˆìŠ¤í† ë¦¬
# ---------------------------------------------------------------------------

def _ensure_backtest_history_table() -> None:
    """backtest_history í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±"""
    db_path = str(_PROJECT_ROOT / DB_PATH)
    conn = sqlite3.connect(db_path)
    conn.execute('''
        CREATE TABLE IF NOT EXISTS backtest_history (
            id            INTEGER PRIMARY KEY AUTOINCREMENT,
            run_at        TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            strategy      TEXT,
            start_date    TEXT,
            end_date      TEXT,
            total_return  REAL,
            mdd           REAL,
            sharpe        REAL,
            calmar        REAL,
            win_rate      REAL,
            total_trades  INTEGER,
            profit_factor REAL,
            min_score     REAL,
            min_signals   INTEGER,
            target_return REAL,
            stop_loss     REAL,
            max_positions INTEGER,
            max_hold_days INTEGER,
            institution_weight REAL,
            note          TEXT DEFAULT '',
            label         TEXT DEFAULT ''
        )
    ''')
    conn.commit()
    conn.close()


# ëª¨ë“ˆ ì„í¬íŠ¸ ì‹œ í…Œì´ë¸” ìë™ ìƒì„±
_ensure_backtest_history_table()


def save_backtest_history(
    result: dict,
    start_date: str,
    end_date: str,
    note: str = '',
    label: str = '',
) -> int:
    """
    ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ íˆìŠ¤í† ë¦¬ DBì— ì €ì¥í•˜ê³  id ë°˜í™˜.

    Parameters
    ----------
    result     : dict   run_backtest() / run_backtest_with_progress() ë°˜í™˜ê°’
    start_date : str    ë°±í…ŒìŠ¤íŠ¸ ì‹œì‘ì¼
    end_date   : str    ë°±í…ŒìŠ¤íŠ¸ ì¢…ë£Œì¼
    note       : str    ì‚¬ìš©ì ë©”ëª¨
    label      : str    ê²°ê³¼ ì‹ë³„ ë ˆì´ë¸”
    """
    metrics = get_metrics_from_result(result)
    cfg = result.get('config', {})
    mdd_info = metrics.max_drawdown() if metrics else {}
    total_trades = len(get_trades_from_result(result))

    db_path = str(_PROJECT_ROOT / DB_PATH)
    conn = sqlite3.connect(db_path)
    cursor = conn.execute('''
        INSERT INTO backtest_history (
            strategy, start_date, end_date,
            total_return, mdd, sharpe, calmar, win_rate,
            total_trades, profit_factor,
            min_score, min_signals, target_return, stop_loss,
            max_positions, max_hold_days, institution_weight,
            note, label
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    ''', (
        cfg.get('strategy', 'long'),
        start_date,
        end_date,
        metrics.total_return()  if metrics else 0.0,
        mdd_info.get('mdd', 0.0),
        metrics.sharpe_ratio()  if metrics else 0.0,
        metrics.calmar_ratio()  if metrics else 0.0,
        metrics.win_rate()      if metrics else 0.0,
        total_trades,
        metrics.profit_factor() if metrics else 0.0,
        cfg.get('min_score', 60),
        cfg.get('min_signals', 1),
        cfg.get('target_return', 0.10),
        cfg.get('stop_loss', -0.05),
        cfg.get('max_positions', 5),
        cfg.get('max_hold_days', 999),
        cfg.get('institution_weight', 0.3),
        note,
        label,
    ))
    row_id = cursor.lastrowid
    conn.commit()
    conn.close()
    return row_id


def get_backtest_history(limit: int = 50) -> pd.DataFrame:
    """ì €ì¥ëœ ë°±í…ŒìŠ¤íŠ¸ íˆìŠ¤í† ë¦¬ ì¡°íšŒ (ìµœì‹ ìˆœ)"""
    db_path = str(_PROJECT_ROOT / DB_PATH)
    conn = sqlite3.connect(db_path)
    df = pd.read_sql_query(
        f"SELECT * FROM backtest_history ORDER BY run_at DESC LIMIT {limit}",
        conn,
    )
    conn.close()
    return df


def delete_backtest_history(row_id: int) -> None:
    """ë°±í…ŒìŠ¤íŠ¸ íˆìŠ¤í† ë¦¬ í–‰ ì‚­ì œ"""
    db_path = str(_PROJECT_ROOT / DB_PATH)
    conn = sqlite3.connect(db_path)
    conn.execute("DELETE FROM backtest_history WHERE id = ?", (row_id,))
    conn.commit()
    conn.close()


# ---------------------------------------------------------------------------
# ê³ ë“ì  ë³€ë™ ì•Œë¦¼ (Score Change Log)
# ---------------------------------------------------------------------------

_SCORE_LOG_TABLE = "score_change_log"
_SCORE_HIGH_THRESHOLD = 70   # "ê³ ë“ì " ê¸°ì¤€


def _ensure_score_change_log_table() -> None:
    """score_change_log í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±"""
    db_path = str(_PROJECT_ROOT / DB_PATH)
    conn = sqlite3.connect(db_path)
    conn.execute(f'''
        CREATE TABLE IF NOT EXISTS {_SCORE_LOG_TABLE} (
            id            INTEGER PRIMARY KEY AUTOINCREMENT,
            logged_at     TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            analysis_date TEXT NOT NULL,
            stock_code    TEXT NOT NULL,
            stock_name    TEXT,
            sector        TEXT,
            pattern       TEXT,
            score         REAL,
            signal_count  INTEGER,
            prev_score    REAL,
            change_type   TEXT  -- 'new_entry', 'score_up', 'score_down', 'exit'
        )
    ''')
    conn.execute(
        f"CREATE INDEX IF NOT EXISTS idx_scl_date ON {_SCORE_LOG_TABLE}(analysis_date DESC)"
    )
    conn.commit()
    conn.close()


# ëª¨ë“ˆ ì„í¬íŠ¸ ì‹œ í…Œì´ë¸” ìë™ ìƒì„±
_ensure_score_change_log_table()


# ì§ì „ ë¶„ì„ ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ìºì‹œ (ëª¨ë“ˆ ë ˆë²¨)
_prev_score_snapshot: Optional[pd.DataFrame] = None


def snapshot_scores(report_df: pd.DataFrame, analysis_date: str) -> None:
    """
    í˜„ì¬ ë¶„ì„ ê²°ê³¼ë¥¼ score_change_logì— ìŠ¤ëƒ…ìƒ· ì €ì¥.
    ì§ì „ ìŠ¤ëƒ…ìƒ·ê³¼ ë¹„êµí•˜ì—¬ ì‹ ê·œ ì§„ì… / ê¸‰ë“± / ì´íƒˆ ì´ë²¤íŠ¸ë¥¼ ê¸°ë¡í•œë‹¤.

    Parameters
    ----------
    report_df     : í˜„ì¬ ë¶„ì„ ê²°ê³¼ DataFrame (get_stage_report ê²°ê³¼)
    analysis_date : YYYY-MM-DD ê¸°ì¤€ì¼ ë¬¸ìì—´
    """
    global _prev_score_snapshot

    db_path = str(_PROJECT_ROOT / DB_PATH)
    conn = sqlite3.connect(db_path)

    # ê°™ì€ analysis_dateë¡œ ì´ë¯¸ ê¸°ë¡ëœ ì´ë²¤íŠ¸ê°€ ìˆìœ¼ë©´ ì¤‘ë³µ ì‚½ì… ë°©ì§€
    existing = conn.execute(
        f"SELECT COUNT(*) FROM {_SCORE_LOG_TABLE} WHERE analysis_date = ?",
        (analysis_date,),
    ).fetchone()[0]
    if existing > 0:
        conn.close()
        return

    # ì§ì „ ìŠ¤ëƒ…ìƒ·: DBì—ì„œ ê°€ì¥ ìµœê·¼ ë‚ ì§œì˜ ê³ ë“ì  ì¢…ëª©
    prev_df = pd.read_sql_query(
        f"""
        SELECT stock_code, score, pattern
        FROM {_SCORE_LOG_TABLE}
        WHERE analysis_date = (
            SELECT MAX(analysis_date)
            FROM {_SCORE_LOG_TABLE}
            WHERE analysis_date < ?
        )
        """,
        conn,
        params=(analysis_date,),
    )
    prev_scores = dict(zip(prev_df['stock_code'], prev_df['score'])) if not prev_df.empty else {}

    # í˜„ì¬ ê³ ë“ì  ì¢…ëª© (threshold ì´ìƒ)
    high_df = report_df[report_df['score'] >= _SCORE_HIGH_THRESHOLD].copy()
    curr_codes = set(high_df['stock_code'].tolist())
    prev_codes = set(prev_scores.keys())

    rows = []
    for _, row in high_df.iterrows():
        code = row['stock_code']
        curr_s = float(row.get('score', 0))
        prev_s = prev_scores.get(code)

        if code not in prev_codes:
            change_type = 'new_entry'
        elif curr_s - (prev_s or 0) >= 5:
            change_type = 'score_up'
        elif (prev_s or 0) - curr_s >= 5:
            change_type = 'score_down'
        else:
            change_type = None  # ë³€ë™ ì—†ìŒ â†’ ë¡œê·¸ ë¶ˆí•„ìš” (ì¤‘ë³µ ë°©ì§€)

        if change_type:
            rows.append((
                analysis_date,
                code,
                str(row.get('stock_name', '')),
                str(row.get('sector', '')),
                str(row.get('pattern', '')),
                curr_s,
                int(row.get('signal_count', 0)),
                prev_s,
                change_type,
            ))

    # ì´íƒˆ ì¢…ëª© (ì§ì „ ê³ ë“ì ì´ì—ˆìœ¼ë‚˜ ì§€ê¸ˆ ì—†ìŒ)
    for code in prev_codes - curr_codes:
        rows.append((
            analysis_date, code, '', '', '', None, 0, prev_scores.get(code), 'exit',
        ))

    if rows:
        conn.executemany(
            f"""INSERT INTO {_SCORE_LOG_TABLE}
            (analysis_date, stock_code, stock_name, sector, pattern,
             score, signal_count, prev_score, change_type)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)""",
            rows,
        )
        conn.commit()

    conn.close()
    _prev_score_snapshot = high_df


def get_score_change_alerts(limit: int = 100) -> pd.DataFrame:
    """
    ìµœê·¼ ê³ ë“ì  ë³€ë™ ì•Œë¦¼ ì¡°íšŒ.
    Returns: DataFrame (analysis_date, change_type, stock_code, stock_name, score, prev_score, ...)
    """
    db_path = str(_PROJECT_ROOT / DB_PATH)
    conn = sqlite3.connect(db_path)
    df = pd.read_sql_query(
        f"""
        SELECT *
        FROM {_SCORE_LOG_TABLE}
        ORDER BY logged_at DESC
        LIMIT {limit}
        """,
        conn,
    )
    conn.close()
    return df
